{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/naver_shopping_tokenized.csv\", encoding = 'utf-8', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['y'].values.tolist()\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(df['text'], y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "def okt_tokenizer(text):\n",
    "    # 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환\n",
    "    tokens_ko = okt.morphs(text)\n",
    "    \n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM 기준, 주요 긍정 단어와 주요 부정 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vect \u001b[39m=\u001b[39m CountVectorizer(tokenizer \u001b[39m=\u001b[39m okt_tokenizer, min_df \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m X_train_tf \u001b[39m=\u001b[39m vect\u001b[39m.\u001b[39;49mfit_transform(X_train_texts)\n\u001b[0;32m      3\u001b[0m X_test_tf \u001b[39m=\u001b[39m vect\u001b[39m.\u001b[39mtransform(X_test_texts)\n",
      "File \u001b[1;32mc:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1263\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1264\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1265\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn [4], line 4\u001b[0m, in \u001b[0;36mokt_tokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mokt_tokenizer\u001b[39m(text):\n\u001b[0;32m      3\u001b[0m     \u001b[39m# 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     tokens_ko \u001b[39m=\u001b[39m okt\u001b[39m.\u001b[39;49mmorphs(text)\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m tokens_ko\n",
      "File \u001b[1;32mc:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmorphs\u001b[39m(\u001b[39mself\u001b[39m, phrase, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, stem\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m [s \u001b[39mfor\u001b[39;00m s, t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos(phrase, norm\u001b[39m=\u001b[39;49mnorm, stem\u001b[39m=\u001b[39;49mstem)]\n",
      "File \u001b[1;32mc:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjki\u001b[39m.\u001b[39;49mtokenize(\n\u001b[0;32m     72\u001b[0m             phrase,\n\u001b[0;32m     73\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(norm),\n\u001b[0;32m     74\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(stem))\u001b[39m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m [t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(tokenizer = okt_tokenizer, min_df = 3, ngram_range=(1,2))\n",
    "X_train_tf = vect.fit_transform(X_train_texts)\n",
    "X_test_tf = vect.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어들을 번호 기준 내림차순으로 저장\n",
    "vocablist = [word for word, number in sorted(vect.vocabulary_.items(), key = lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_DTM = LogisticRegression(random_state = 0, solver = 'liblinear', C = 1)\n",
    "lr_DTM.fit(X_train_tf, y_train)\n",
    "y_pred_DTM = lr_DTM.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)\n",
      "맛있습니다(2.755)\n",
      "맛있네요(2.703)\n",
      "이쁩니다(2.661)\n",
      "맛있고(2.497)\n",
      "맛있어요(2.496)\n",
      "만족스럽습니다(2.479)\n",
      "튼튼합니다(2.303)\n",
      "잘 왔어요(2.270)\n",
      "최고(2.252)\n",
      "좋았습니다(2.238)\n",
      "\n",
      "부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)\n",
      "최악(-3.713)\n",
      "실망(-2.971)\n",
      "좋네요 좋네요(-2.902)\n",
      "비추(-2.780)\n",
      "맛있어요 맛있어요(-2.680)\n",
      "별루(-2.674)\n",
      "실망했어요(-2.640)\n",
      "만족합니다 만족합니다(-2.617)\n",
      "불편해요(-2.610)\n",
      "굿 ㅎ(-2.492)\n"
     ]
    }
   ],
   "source": [
    "coefficients = lr_DTM.coef_.tolist()\n",
    "\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "# coefficients(계수)가 큰 값부터 내림차순으로 정렬\n",
    "\n",
    "print('▶긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)◀')\n",
    "for word_num, coef in sorted_coefficients[:10]:\n",
    "    print('{0:}({1:.3f})'.format(vocablist[word_num], coef))\n",
    "\n",
    "print('\\n▶부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)◀')\n",
    "for word_num, coef in sorted_coefficients[-10:][::-1]: \n",
    "    print('{0:}({1:.3f})'.format(vocablist[word_num], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 기준, 주요 긍정 단어와 주요 부정 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df = 3, max_df = 0.9, tokenizer = okt_tokenizer)\n",
    "tfidf_vectorizer.fit(X_train_texts)\n",
    "tfidf_matrix_train = tfidf_vectorizer.transform(X_train_texts)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocablist2 = [word for word, number in sorted(tfidf_vectorizer.vocabulary_.items(), key = lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=3.5, random_state=0, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=3.5, random_state=0, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=3.5, random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf = LogisticRegression(random_state = 0, solver = 'liblinear', C = 3.5)\n",
    "lr_tfidf.fit(tfidf_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)\n",
      "맛있어요(8.237)\n",
      "좋네요(7.898)\n",
      "좋고(7.572)\n",
      "좋아요(7.396)\n",
      "좋습니다(7.313)\n",
      "예뻐요(6.687)\n",
      "깔끔하고(6.643)\n",
      "맛있네요(6.573)\n",
      "빠르고(6.478)\n",
      "걱정했는데(6.396)\n",
      "\n",
      "부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)\n",
      "별로에요(-10.027)\n",
      "별로네요(-9.625)\n",
      "실망입니다(-8.191)\n",
      "별로입니다(-7.532)\n",
      "별로예요(-7.155)\n",
      "불편해요(-7.002)\n",
      "실망(-6.814)\n",
      "다시는(-6.634)\n",
      "최악(-6.547)\n",
      "버렸어요(-6.546)\n"
     ]
    }
   ],
   "source": [
    "coefficients2 = lr_tfidf.coef_.tolist()\n",
    "\n",
    "sorted_coefficients2 = sorted(enumerate(coefficients2[0]), key = lambda x:x[1], reverse = True)\n",
    "\n",
    "\n",
    "print('▶긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)◀')\n",
    "for word_num, coef in sorted_coefficients2[:10]:\n",
    "    print('{0:}({1:.3f})'.format(vocablist2[word_num], coef))\n",
    "\n",
    "print('\\n▶부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)◀')\n",
    "for word_num, coef in sorted_coefficients2[-10:][::-1]: \n",
    "    print('{0:}({1:.3f})'.format(vocablist2[word_num], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8980de4a27bf89e986bf71fe1fbb5d572934705242a5da409cea5b4049808f61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
