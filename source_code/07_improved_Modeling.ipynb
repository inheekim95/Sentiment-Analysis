{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링에 사용할 파일\n",
    "* 불러오기\n",
    "* null 값 있는지 확인하기\n",
    "* label 형성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/naver_shopping_preprocessed.csv\", encoding = 'utf-8', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score     0\n",
       "Review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>배공빠르고 굿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 박음질이 조금 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>민트색상상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             Review\n",
       "0      5                                            배공빠르고 굿\n",
       "1      2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고\n",
       "2      5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 박음질이 조금 ...\n",
       "3      2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...\n",
       "4      5                 민트색상상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_to_label(rating):\n",
    "    if rating >=4 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['y'] = df['Score'].apply(lambda x: rating_to_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 사전\n",
    "* 기본적인 불용어 사전 불러오기\n",
    "* 자체적으로 추가한 불용어 사전 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for s in stopwords:\n",
    "    temp.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../data/stopwords_self.txt\", 'r', encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = f.readlines()\n",
    "for idx, l in enumerate(lines):\n",
    "    lines[idx] = l.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['지금', '입다', '입히다', '음', '욤']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = []\n",
    "for l in lines:\n",
    "    temp2.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= list(set(temp).union(set(temp2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['앞의것', '까지 미치다', '콸콸', '잠', '남']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화 함수 정의\n",
    "* 명사, 형용사, 동사 + 1글자 이상만 추출하는 경우 - tokenizer_1\n",
    "* 형태소 모두 사용 - tokenizer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_1(text):\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]')\n",
    "    result = hangul.sub('', text)\n",
    "    okt = Okt()\n",
    "    Okt_morphs = okt.pos(result)\n",
    "    words = []\n",
    "\n",
    "    for word, pos in Okt_morphs:\n",
    "        if pos == 'Adjective' or pos == 'Verb' or pos == 'Noun':\n",
    "            if len(word) > 1 and word not in stopwords:\n",
    "                words.append(word)\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt2 = Okt()\n",
    "def tokenizer_2(text):\n",
    "    tokens_ko = okt2.morphs(text)\n",
    "\n",
    "    result = []\n",
    "    for word in tokens_ko:\n",
    "        if word not in stopwords:\n",
    "            result.append(word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아버지 가방 들어가신다'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_1(\"아버지 가방에 들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아버지 가방 들어가신다'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_2(\"아버지 가방에 들어가신다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized 된 컬럼을 df에 추가\n",
    "* 원래는 CounterVectorizer나 TFIDF 하이퍼파라미터에 해당 tokenizer_1, tokenizer_2 함수를 tokenizer로 지정하는 방법이 기본적인 방법\n",
    "* 시간을 효율적으로 쓰기 위해 tokenizer_1, tokenizer_2 함수를 적용한 컬럼 tokenized_1, tokenized_2를 df에 추가함\n",
    "* CounterVectorizer와 TFIDF함수에 이미 토큰화된 컬럼을 테스트셋, 훈련용셋으로 나눠서 사용하고 tokenizer는 띄어쓰기 기준으로 split하는 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200000it [1:19:32, 41.91it/s] \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_1 = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    tokenized_review = tokenizer_1(df.loc[idx, 'Review'])\n",
    "    tokenized_1.append(tokenized_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_1'] = tokenized_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200000it [10:34, 315.20it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_2 = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    tokenized_review = tokenizer_2(df.loc[idx, 'Review'])\n",
    "    tokenized_2.append(tokenized_review)\n",
    "\n",
    "df['tokenized_2'] = tokenized_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Review</th>\n",
       "      <th>y</th>\n",
       "      <th>tokenized_1</th>\n",
       "      <th>tokenized_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>배공빠르고 굿</td>\n",
       "      <td>1</td>\n",
       "      <td>배공 빠르고</td>\n",
       "      <td>배공 빠르고 굿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고</td>\n",
       "      <td>0</td>\n",
       "      <td>택배 엉망 놔두고가고</td>\n",
       "      <td>택배 엉망 이네 용 집 밑 층 없이 놔두고가고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 박음질이 조금 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>아주 좋아요 바지 정말 좋아서 구매 가격 대박 입니다 박음질 어설프다하긴 편하고 가...</td>\n",
       "      <td>아주 좋아요 바지 정말 좋아서 2 구매 가격 대박 입니다 . 박음질 어설프다하긴 편...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...</td>\n",
       "      <td>0</td>\n",
       "      <td>선물 받아서 전달 했어야 하는 상품 이었는데 머그컵 와서 당황 했습니다 전화했더니 ...</td>\n",
       "      <td>선물 용 빨리 받아서 전달 했어야 하는 상품 이었는데 머그컵 와서 당황 했습니다 ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>민트색상상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ</td>\n",
       "      <td>1</td>\n",
       "      <td>민트 색상 예뻐요 손잡이 도로 사용 되네요</td>\n",
       "      <td>민트 색상 예뻐요 . 손잡이 는 는 용 도로 사용 되네요 ㅎㅎ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                             Review  y  \\\n",
       "0      5                                            배공빠르고 굿  1   \n",
       "1      2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고  0   \n",
       "2      5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 박음질이 조금 ...  1   \n",
       "3      2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...  0   \n",
       "4      5                 민트색상상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ  1   \n",
       "\n",
       "                                         tokenized_1  \\\n",
       "0                                             배공 빠르고   \n",
       "1                                        택배 엉망 놔두고가고   \n",
       "2  아주 좋아요 바지 정말 좋아서 구매 가격 대박 입니다 박음질 어설프다하긴 편하고 가...   \n",
       "3  선물 받아서 전달 했어야 하는 상품 이었는데 머그컵 와서 당황 했습니다 전화했더니 ...   \n",
       "4                            민트 색상 예뻐요 손잡이 도로 사용 되네요   \n",
       "\n",
       "                                         tokenized_2  \n",
       "0                                           배공 빠르고 굿  \n",
       "1                          택배 엉망 이네 용 집 밑 층 없이 놔두고가고  \n",
       "2  아주 좋아요 바지 정말 좋아서 2 구매 가격 대박 입니다 . 박음질 어설프다하긴 편...  \n",
       "3  선물 용 빨리 받아서 전달 했어야 하는 상품 이었는데 머그컵 와서 당황 했습니다 ....  \n",
       "4                 민트 색상 예뻐요 . 손잡이 는 는 용 도로 사용 되네요 ㅎㅎ  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/naver_shopping_tokenized_review.csv\", sep = \",\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score          0\n",
      "Review         0\n",
      "y              0\n",
      "tokenized_1    0\n",
      "tokenized_2    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200000 entries, 0 to 199999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   Score        200000 non-null  int64 \n",
      " 1   Review       200000 non-null  object\n",
      " 2   y            200000 non-null  int64 \n",
      " 3   tokenized_1  200000 non-null  object\n",
      " 4   tokenized_2  200000 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 13.2+ MB\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM + GridSearchCV + tokenizer_1\n",
    "* countervectorizer\n",
    "* GridSearchCV\n",
    "* Tokenizer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['y']\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(df['tokenized_1'], y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df = 3, ngram_range=(1,2), tokenizer = tokenizer)\n",
    "X_train_tf = vect.fit_transform(X_train_texts)\n",
    "X_test_tf = vect.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocablist_DTM_tk1 = [word for word, number in sorted(vect.vocabulary_.items(), key = lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1} 0.8942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression(random_state = 0, solver = 'liblinear')\n",
    "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "grid_cv = GridSearchCV(lr , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
    "grid_cv.fit(X_train_tf , y_train)\n",
    "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_DTM_tk1 = grid_cv.best_estimator_\n",
    "y_pred = best_estimator_DTM_tk1.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.90\n",
      "Precision : 0.901\n",
      "Recall : 0.891\n",
      "F1 : 0.896\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
    "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
    "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('DTM_tokenizer1_gridsearch_done.pickle','wb') as fw:\n",
    "    pickle.dump(best_estimator_DTM_tk1, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_estimator_DTM_tk1.coef_.tolist()\n",
    "\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "# coefficients(계수)가 큰 값부터 내림차순으로 정렬\n",
    "\n",
    "print('긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[:10]:\n",
    "    print('{0:}({1:.3f})'.format(vocablist_DTM_tk1[word_num], coef))\n",
    "\n",
    "print('\\n부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[-10:][::-1]: \n",
    "    print('{0:}({1:.3f})'.format(vocablist_DTM_tk1[word_num], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF + GridSearchCV + tokenizer_1\n",
    "* TFIDF\n",
    "* GridSearchCV\n",
    "* Tokenizer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df = 3, max_df = 0.9, tokenizer = tokenizer)\n",
    "tfidf_vectorizer.fit(X_train_texts)\n",
    "tfidf_matrix_train = tfidf_vectorizer.transform(X_train_texts)\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocablist_TFIDF_tk1 = [word for word, number in sorted(tfidf_vectorizer.vocabulary_.items(), key = lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "{'C': 3.5} 0.8965\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state = 0, solver = 'liblinear')\n",
    "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "grid_cv = GridSearchCV(lr , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
    "grid_cv.fit(tfidf_matrix_train , y_train)\n",
    "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator_TFIDF_tk1 = grid_cv.best_estimator_\n",
    "y_pred = best_estimator_TFIDF_tk1.predict(tfidf_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.90\n",
      "Precision : 0.901\n",
      "Recall : 0.893\n",
      "F1 : 0.897\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
    "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
    "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TFIDF_tokenizer1_gridsearch_done.pickle','wb') as fw:\n",
    "    pickle.dump(best_estimator_TFIDF_tk1, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_estimator_TFIDF_tk1.coef_.tolist()\n",
    "\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "# coefficients(계수)가 큰 값부터 내림차순으로 정렬\n",
    "\n",
    "print('긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[:10]:\n",
    "    print('{0:}({1:.3f})'.format(vocablist_TFIDF_tk1[word_num], coef))\n",
    "\n",
    "print('\\n부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[-10:][::-1]: \n",
    "    print('{0:}({1:.3f})'.format(vocablist_TFIDF_tk1[word_num], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM + GridSearchCV + tokenizer_2\n",
    "* countervectorizer\n",
    "* GridSearchCV\n",
    "* Tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(df['tokenized_2'], y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vect2 = CountVectorizer(min_df = 3, ngram_range=(1,2), tokenizer = tokenizer)\n",
    "X_train_tf = vect2.fit_transform(X_train_texts)\n",
    "X_test_tf = vect2.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocablist_DTM_tk2 = [word for word, number in sorted(vect2.vocabulary_.items(), key = lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\taeri\\miniconda3\\envs\\ds_study\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1} 0.9095\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state = 0, solver = 'liblinear')\n",
    "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "grid_cv = GridSearchCV(lr , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
    "grid_cv.fit(X_train_tf , y_train)\n",
    "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.91\n",
      "Precision : 0.910\n",
      "Recall : 0.909\n",
      "F1 : 0.909\n"
     ]
    }
   ],
   "source": [
    "best_estimator_DTM_tk2 = grid_cv.best_estimator_\n",
    "y_pred = best_estimator_DTM_tk2.predict(X_test_tf)\n",
    "\n",
    "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
    "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
    "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DTM_tokenizer2_gridsearch_done.pickle','wb') as fw:\n",
    "    pickle.dump(best_estimator_TFIDF_tk2, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_estimator_DTM_tk2.coef_.tolist()\n",
    "\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "# coefficients(계수)가 큰 값부터 내림차순으로 정렬\n",
    "\n",
    "print('긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[:10]:\n",
    "    print('{0:}({1:.3f})'.format(vocablist_DTM_tk2[word_num], coef))\n",
    "\n",
    "print('\\n부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[-10:][::-1]: \n",
    "    print('{0:}({1:.3f})'.format(vocablist_DTM_tk2[word_num], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + GridSearchCV + tokenizer_2\n",
    "* TFIDF\n",
    "* GridSearchCV\n",
    "* Tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer2 = TfidfVectorizer(ngram_range=(1,2), min_df = 3, max_df = 0.9, tokenizer = tokenizer)\n",
    "tfidf_vectorizer2.fit(X_train_texts)\n",
    "tfidf_matrix_train = tfidf_vectorizer2.transform(X_train_texts)\n",
    "tfidf_matrix_test = tfidf_vectorizer2.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocablist_TFIDF_tk2 = [word for word, number in sorted(vect2.vocabulary_.items(), key = lambda x:x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "{'C': 3.5} 0.9123\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state = 0, solver = 'liblinear')\n",
    "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
    "grid_cv = GridSearchCV(lr , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
    "grid_cv.fit(tfidf_matrix_train, y_train)\n",
    "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.90\n",
      "Precision : 0.882\n",
      "Recall : 0.928\n",
      "F1 : 0.904\n"
     ]
    }
   ],
   "source": [
    "best_estimator_TFIDF_tk2 = grid_cv.best_estimator_\n",
    "y_pred = best_estimator_TFIDF_tk2.predict(X_test_tf)\n",
    "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
    "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
    "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TFIDF_tokenizer2_gridsearch_done.pickle','wb') as fw:\n",
    "    pickle.dump(best_estimator_TFIDF_tk2, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = best_estimator_TFIDF_tk2.coef_.tolist()\n",
    "\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "# coefficients(계수)가 큰 값부터 내림차순으로 정렬\n",
    "\n",
    "print('긍정적인 단어 Top 10 (높은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[:10]:\n",
    "    print('{0:}({1:.3f})'.format(vocablist_TFIDF_tk2[word_num], coef))\n",
    "\n",
    "print('\\n부정적인 단어 Top 10 (낮은 평점과 상관관계가 강한 단어들)')\n",
    "for word_num, coef in sorted_coefficients[-10:][::-1]: \n",
    "    print('{0:}({1:.3f})'.format(vocablist_TFIDF_tk2[word_num], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM + 하이퍼파라미터 튜닝 + 유의어 일원화 + 명사/형용사/동사만 추출\n",
    "* accuracy: 0.90\n",
    "* Precision : 0.901\n",
    "* Recall : 0.891\n",
    "* F1 : 0.896\n",
    "\n",
    "\n",
    "## TFIDF + 하이퍼파라미터 튜닝 + 유의어 일원화 + 명사/형용사/동사만 추출\n",
    "* accuracy: 0.90\n",
    "* Precision : 0.901\n",
    "* Recall : 0.893\n",
    "* F1 : 0.897\n",
    "\n",
    "## DTM + 하이퍼파라미터 튜닝 + 유의어 일원화 + 형태소만 추출\n",
    "* accuracy: 0.91\n",
    "* Precision : 0.910\n",
    "* Recall : 0.909\n",
    "* F1 : 0.909\n",
    "\n",
    "\n",
    "## TFIDF + 하이퍼파라미터 튜닝 + 유의어 일원화 + 형태소만 추출\n",
    "* accuracy: 0.90\n",
    "* Precision : 0.882\n",
    "* Recall : 0.928\n",
    "* F1 : 0.904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8980de4a27bf89e986bf71fe1fbb5d572934705242a5da409cea5b4049808f61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
